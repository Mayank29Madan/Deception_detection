{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Task Overview**#\n",
        "\n",
        "The goal of this task is to detect deception in dialogues by training a logistic regression model using stochastic gradient descent (SGD). The dataset consists of text messages from a game where each message is labeled as truthful or deceptive.\n",
        "\n",
        "##**Two text vectorization methods are used:**##\n",
        "\n",
        "CountVectorizer (word frequency-based representation)\n",
        "\n",
        "TF-IDF Vectorizer (importance-based representation)\n",
        "\n",
        "**The model is trained separately for both vectorization methods, and their performance is compared.**\n"
      ],
      "metadata": {
        "id": "3fQaLJJXlhi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Installation of required files**#"
      ],
      "metadata": {
        "id": "UOXBHFDV9Te_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First install any required packages that aren't pre-installed in Colab\n",
        "!pip install jsonlines spacy scikit-learn gdown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1onvUdqYnjAW",
        "outputId": "4304b656-4ce4-4b51-d752-66a2afd83003"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jsonlines\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonlines) (25.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown) (3.18.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-4.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Importing necessary libraries**#"
      ],
      "metadata": {
        "id": "hPJdGZIv9e1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jsonlines\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import log_loss, accuracy_score, f1_score\n",
        "from scipy.sparse import csr_matrix\n",
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "import os\n",
        "import gdown\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "30wX429cl5bG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Dataset and Configuration**#\n",
        "The dataset is stored in JSONL format and is downloaded from Google Drive if not available.\n",
        "\n",
        "The dataset contains game dialogues, where players communicate with each other.\n",
        "\n",
        "Each message is labeled as truthful or deceptive (sender_annotation or receiver_annotation).\n",
        "\n",
        "The task can be configured to focus on either:\n",
        "\n",
        "**SENDER**: Whether the sender is truthful or deceptive.\n",
        "\n",
        "**RECEIVER**: Whether the receiver perceives the message as truthful or deceptive.\n",
        "\n",
        "The \"**POWER**\" setting determines whether the model considers game score changes as additional features."
      ],
      "metadata": {
        "id": "H-h4UFNYmJQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TASK = \"SENDER\"  # Can be \"SENDER\" or \"RECEIVER\"\n",
        "POWER = \"y\"      # Can be \"y\" (include power) or \"n\" (exclude power)\n",
        "\n",
        "# File IDs extracted from Google Drive links\n",
        "FILE_IDS = {\n",
        "    'train.jsonl': '1sHVikR018z4QEBmHPUVwRXoKmgv7G69q',\n",
        "    'validation.jsonl': '1Y-MKTjzrrP8X8oTxB8yz9awilRsXSNS_',\n",
        "    'test.jsonl': '1sDgZ98_OXsoCt5WxroXLSlwMcCUZp4zm'\n",
        "}\n",
        "\n",
        "DATA_DIR = 'data/'\n",
        "\n",
        "print(f\"Configuration: TASK={TASK}, POWER={POWER}\")\n",
        "def download_files():\n",
        "    # Create data directory if it doesn't exist\n",
        "    os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "    # Download each file if it doesn't already exist\n",
        "    for filename, file_id in FILE_IDS.items():\n",
        "        filepath = os.path.join(DATA_DIR, filename)\n",
        "        if not os.path.exists(filepath):\n",
        "            url = f'https://drive.google.com/uc?id={file_id}'\n",
        "            gdown.download(url, filepath, quiet=False)\n",
        "            print(f\"Downloaded {filename}\")\n",
        "        else:\n",
        "            print(f\"{filename} already exists, skipping download\")\n",
        "\n",
        "# Download the files\n",
        "download_files()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1erSE_aymAVA",
        "outputId": "842a2812-d438-4587-e524-8cf693772317"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration: TASK=SENDER, POWER=y\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1sHVikR018z4QEBmHPUVwRXoKmgv7G69q\n",
            "To: /content/data/train.jsonl\n",
            "100%|██████████| 2.47M/2.47M [00:00<00:00, 18.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded train.jsonl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Y-MKTjzrrP8X8oTxB8yz9awilRsXSNS_\n",
            "To: /content/data/validation.jsonl\n",
            "100%|██████████| 246k/246k [00:00<00:00, 4.71MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded validation.jsonl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1sDgZ98_OXsoCt5WxroXLSlwMcCUZp4zm\n",
            "To: /content/data/test.jsonl\n",
            "100%|██████████| 490k/490k [00:00<00:00, 6.69MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded test.jsonl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT Transformer implementation"
      ],
      "metadata": {
        "id": "x8uYhgcBBudh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertTokenizer, BertModel, AutoModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import json\n",
        "from typing import List, Dict, Optional\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "from tqdm import tqdm\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "class DiplomacyDataset(Dataset):\n",
        "    def __init__(self, filepath: str, tokenizer: BertTokenizer, max_len: int = 64):\n",
        "        self.instances = []\n",
        "        self.tokenizer = tokenizer\n",
        "        with open(filepath, 'r') as f:\n",
        "            for line in f:\n",
        "                data = json.loads(line)\n",
        "                if len(data['messages']) == 0:\n",
        "                    continue\n",
        "\n",
        "                msgs, labels = [], []\n",
        "                for msg, label in zip(data['messages'], data['sender_labels']):\n",
        "                    if str(label).lower() not in ['true', 'false']:\n",
        "                        continue\n",
        "                    msg = msg.lower().strip()\n",
        "                    msgs.append(msg)\n",
        "                    labels.append(1 if str(label).lower() == 'true' else 0)\n",
        "\n",
        "                if msgs:\n",
        "                    self.instances.append((msgs, labels))\n",
        "\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.instances)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        messages, labels = self.instances[idx]\n",
        "        # Process only up to 10 messages per instance to save memory\n",
        "        messages = messages[:10]\n",
        "        labels = labels[:10]\n",
        "\n",
        "        encoded = [self.tokenizer(\n",
        "            msg,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        ) for msg in messages]\n",
        "\n",
        "        input_ids = torch.cat([e['input_ids'] for e in encoded], dim=0)\n",
        "        attention_mask = torch.cat([e['attention_mask'] for e in encoded], dim=0)\n",
        "        labels = torch.tensor(labels, dtype=torch.long)\n",
        "        return input_ids, attention_mask, labels\n",
        "\n",
        "class BertPooler(nn.Module):\n",
        "    def __init__(self, model_name='bert-base-uncased', trainable=True):\n",
        "        super().__init__()\n",
        "        # Use gradient checkpointing to save memory\n",
        "        self.bert = AutoModel.from_pretrained(model_name)\n",
        "        if trainable:\n",
        "            self.bert.gradient_checkpointing_enable()\n",
        "        for p in self.bert.parameters():\n",
        "            p.requires_grad = trainable\n",
        "        self.hidden_size = self.bert.config.hidden_size\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        with torch.cuda.amp.autocast():\n",
        "            output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        return output.last_hidden_state[:, 0]\n",
        "\n",
        "class HierarchicalLSTM(nn.Module):\n",
        "    def __init__(self, pos_weight: float = 1.0, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.bert_pooler = BertPooler()\n",
        "\n",
        "        # Smaller LSTM to save memory\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.bert_pooler.hidden_size,\n",
        "            hidden_size=128,  # Reduced from 256\n",
        "            num_layers=1,    # Reduced from 2\n",
        "            bidirectional=True,\n",
        "            batch_first=True,\n",
        "            dropout=0\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(128 * 2, 64),  # Reduced dimensions\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(64, 2)\n",
        "        )\n",
        "\n",
        "        self._init_weights()\n",
        "        self.pos_weight = torch.tensor([1.0, pos_weight])\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for name, module in self.named_modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.xavier_normal_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.constant_(module.bias, 0)\n",
        "            elif isinstance(module, nn.LSTM):\n",
        "                for name, param in module.named_parameters():\n",
        "                    if 'weight' in name:\n",
        "                        nn.init.orthogonal_(param)\n",
        "                    elif 'bias' in name:\n",
        "                        nn.init.constant_(param, 0)\n",
        "                        n = param.size(0)\n",
        "                        param.data[n//4:n//2].fill_(1.0)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        B, M, T = input_ids.shape\n",
        "        input_ids = input_ids.view(B * M, T)\n",
        "        attention_mask = attention_mask.view(B * M, T)\n",
        "\n",
        "        message_vecs = self.bert_pooler(input_ids, attention_mask)\n",
        "        message_vecs = self.dropout(message_vecs).view(B, M, -1)\n",
        "\n",
        "        lstm_out, _ = self.lstm(message_vecs)\n",
        "        lstm_out = self.dropout(lstm_out)\n",
        "\n",
        "        logits = self.classifier(lstm_out)\n",
        "\n",
        "        output = {'logits': logits}\n",
        "        if labels is not None:\n",
        "            logits_flat = logits.view(-1, 2)\n",
        "            labels_flat = labels.view(-1)\n",
        "            mask = labels_flat != -1\n",
        "            loss = F.cross_entropy(\n",
        "                logits_flat[mask],\n",
        "                labels_flat[mask],\n",
        "                weight=self.pos_weight.to(logits.device),\n",
        "                label_smoothing=0.1\n",
        "            )\n",
        "            output['loss'] = loss\n",
        "\n",
        "        return output\n",
        "\n",
        "def collate_batch(batch):\n",
        "    max_messages = min(10, max([x[0].size(0) for x in batch]))  # Cap at 10 messages\n",
        "    B = len(batch)\n",
        "    T = batch[0][0].size(1)\n",
        "\n",
        "    input_ids = torch.zeros(B, max_messages, T, dtype=torch.long)\n",
        "    attention_mask = torch.zeros_like(input_ids)\n",
        "    labels = torch.full((B, max_messages), -1, dtype=torch.long)\n",
        "\n",
        "    for i, (ids, mask, lbl) in enumerate(batch):\n",
        "        m = min(ids.size(0), max_messages)\n",
        "        input_ids[i, :m] = ids[:m]\n",
        "        attention_mask[i, :m] = mask[:m]\n",
        "        labels[i, :m] = lbl[:m]\n",
        "\n",
        "    return input_ids, attention_mask, labels\n",
        "\n",
        "def train_model(train_file, val_file, epochs=10, batch_size=4, pos_weight=15):  # Reduced batch_size\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    train_data = DiplomacyDataset(train_file, tokenizer)\n",
        "    val_data = DiplomacyDataset(val_file, tokenizer)\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = HierarchicalLSTM(pos_weight=pos_weight).to(device)\n",
        "\n",
        "    # Freeze first few BERT layers to save memory\n",
        "    for param in list(model.bert_pooler.bert.encoder.layer[:6].parameters()):\n",
        "        param.requires_grad = False\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='max', factor=0.5, patience=2, verbose=True\n",
        "    )\n",
        "\n",
        "    best_f1 = 0\n",
        "    patience = 3\n",
        "    no_improve = 0\n",
        "\n",
        "    scaler = GradScaler()\n",
        "    accumulation_steps = max(1, 32 // batch_size)  # Increased accumulation steps\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for i, (input_ids, attention_mask, labels) in enumerate(tqdm(train_loader)):\n",
        "            input_ids = input_ids.to(device, non_blocking=True)\n",
        "            attention_mask = attention_mask.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            with autocast():\n",
        "                output = model(input_ids, attention_mask, labels)\n",
        "                loss = output['loss'] / accumulation_steps\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            if (i + 1) % accumulation_steps == 0:\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            total_loss += loss.item() * accumulation_steps\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        preds, trues = [], []\n",
        "        with torch.no_grad():\n",
        "            for input_ids, attention_mask, labels in val_loader:\n",
        "                input_ids = input_ids.to(device, non_blocking=True)\n",
        "                attention_mask = attention_mask.to(device, non_blocking=True)\n",
        "\n",
        "                with autocast():\n",
        "                    output = model(input_ids, attention_mask)\n",
        "\n",
        "                logits = output['logits']\n",
        "                pred = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "                labels = labels.numpy()\n",
        "\n",
        "                for p, l in zip(pred, labels):\n",
        "                    for pi, li in zip(p, l):\n",
        "                        if li != -1:\n",
        "                            preds.append(pi)\n",
        "                            trues.append(li)\n",
        "\n",
        "        f1 = f1_score(trues, preds, average='macro')\n",
        "        accuracy = accuracy_score(trues, preds)\n",
        "        precision = precision_score(trues, preds)\n",
        "        recall = recall_score(trues, preds)\n",
        "\n",
        "        print(f\"[Epoch {epoch+1}] Train Loss: {total_loss/len(train_loader):.4f}\")\n",
        "        print(f\"Validation - F1: {f1:.4f}, Acc: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
        "\n",
        "        scheduler.step(f1)\n",
        "\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            no_improve = 0\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "            print(\"New best model saved!\")\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            if no_improve >= patience:\n",
        "                print(f\"No improvement for {patience} epochs, stopping early\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(torch.load('best_model.pt'))\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Set environment variable to help with memory fragmentation\n",
        "    import os\n",
        "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "    trained_model = train_model(\n",
        "        train_file=\"data/train.jsonl\",\n",
        "        val_file=\"data/validation.jsonl\",\n",
        "        epochs=15,\n",
        "        batch_size=1,  # Reduced from 8\n",
        "        pos_weight=15\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECPMpm0x-mOD",
        "outputId": "aa2e040a-4ae5-4389-dfa8-14e2d2b2e24a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "<ipython-input-1-35665d26aab5>:187: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "  0%|          | 0/184 [00:00<?, ?it/s]<ipython-input-1-35665d26aab5>:200: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "<ipython-input-1-35665d26aab5>:70: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "100%|██████████| 184/184 [00:12<00:00, 14.34it/s]\n",
            "<ipython-input-1-35665d26aab5>:224: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] Train Loss: 0.1983\n",
            "Validation - F1: 0.4804, Acc: 0.9246, Precision: 0.9246, Recall: 1.0000\n",
            "New best model saved!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/184 [00:00<?, ?it/s]<ipython-input-1-35665d26aab5>:200: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "<ipython-input-1-35665d26aab5>:70: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "100%|██████████| 184/184 [00:11<00:00, 16.72it/s]\n",
            "<ipython-input-1-35665d26aab5>:224: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 2] Train Loss: 0.0779\n",
            "Validation - F1: 0.4804, Acc: 0.9246, Precision: 0.9246, Recall: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/184 [00:00<?, ?it/s]<ipython-input-1-35665d26aab5>:200: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "<ipython-input-1-35665d26aab5>:70: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "100%|██████████| 184/184 [00:10<00:00, 17.00it/s]\n",
            "<ipython-input-1-35665d26aab5>:224: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 3] Train Loss: 0.0788\n",
            "Validation - F1: 0.4804, Acc: 0.9246, Precision: 0.9246, Recall: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/184 [00:00<?, ?it/s]<ipython-input-1-35665d26aab5>:200: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "<ipython-input-1-35665d26aab5>:70: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "100%|██████████| 184/184 [00:10<00:00, 16.90it/s]\n",
            "<ipython-input-1-35665d26aab5>:224: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 4] Train Loss: 0.0726\n",
            "Validation - F1: 0.4804, Acc: 0.9246, Precision: 0.9246, Recall: 1.0000\n",
            "No improvement for 3 epochs, stopping early\n"
          ]
        }
      ]
    }
  ]
}