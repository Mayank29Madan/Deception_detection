{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0CLpY5u61u9",
        "outputId": "859a9d8f-5458-4165-d669-459392091378"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.19.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIvzd2mT6wyJ",
        "outputId": "3c9f3a1e-1ea8-46cc-df96-fd94c719ac2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading data...\n",
            "Augmenting training data...\n",
            "Original data - Truthful: 591, Deceptive: 12541\n",
            "Minority class: Truthful\n",
            "Creating 21 augmentations per minority class message\n",
            "After augmentation - Truthful: 12456, Deceptive: 12541\n",
            "Building player mappings...\n",
            "Building TF-IDF and BOW features...\n",
            "Building player interaction graphs...\n",
            "Loading tokenizer...\n",
            "Creating datasets...\n",
            "Initializing model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 3125/3125 [07:07<00:00,  7.30it/s]\n",
            "Validation: 100%|██████████| 177/177 [00:13<00:00, 13.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0373, Train Acc: 0.6914, Train F1: 0.6914\n",
            "Val Loss: 0.0298, Val Acc: 0.8319, Val F1: 0.5223\n",
            "Saving new best model...\n",
            "--------------------------------------------------\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 3125/3125 [07:06<00:00,  7.32it/s]\n",
            "Validation: 100%|██████████| 177/177 [00:13<00:00, 13.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0135, Train Acc: 0.9356, Train F1: 0.9356\n",
            "Val Loss: 0.0224, Val Acc: 0.9435, Val F1: 0.5093\n",
            "--------------------------------------------------\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  42%|████▏     | 1309/3125 [02:57<04:00,  7.55it/s]"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch_geometric.nn import GCNConv, GATConv, GATv2Conv\n",
        "from torch_geometric.data import Data\n",
        "from transformers import DistilBertTokenizer, DistilBertModel, AutoModel, RobertaTokenizer, RobertaModel\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, precision_recall_curve, roc_auc_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict, Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "# Set seed for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Enhanced constants\n",
        "MAX_LEN = 160  # Increased for more context\n",
        "BATCH_SIZE = 8  # Reduced for better gradient updates\n",
        "EPOCHS = 10  # Increased training time\n",
        "LEARNING_RATE = 2e-5  # Fine-tuned learning rate\n",
        "NUM_PREVIOUS_MESSAGES = 3  # Increased context window\n",
        "PLAYER_EMBEDDING_DIM = 64  # Increased complexity\n",
        "METADATA_DIM = 32  # Increased metadata representation\n",
        "DROPOUT_RATE = 0.4  # Tuned dropout\n",
        "\n",
        "# Improved text preprocessing\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Enhanced text preprocessing with better feature extraction\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Replace multiple spaces with single space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Standardize punctuation spacing\n",
        "    text = re.sub(r'([.,!?;:])', r' \\1 ', text)\n",
        "\n",
        "    # Standardize quotes\n",
        "    text = re.sub(r'[\"\"\"]', '\"', text)\n",
        "    text = re.sub(r\"[‘’´`']\", \"'\", text)\n",
        "\n",
        "\n",
        "    # Replace repeated characters (like \"sooooo\" -> \"soo\")\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
        "\n",
        "    # Final cleaning\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Enhanced linguistic features extraction\n",
        "def extract_linguistic_features(text):\n",
        "    \"\"\"Extract advanced linguistic features that may indicate deception\"\"\"\n",
        "    if not text:\n",
        "        return np.zeros(10)\n",
        "\n",
        "    # Tokenize text\n",
        "    tokens = text.lower().split()\n",
        "\n",
        "    # Get stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    features = [\n",
        "        # Message length (normalized)\n",
        "        min(1.0, len(tokens) / 100.0),\n",
        "\n",
        "        # Question marks (may indicate seeking information or creating doubt)\n",
        "        min(1.0, text.count('?') / 5.0),\n",
        "\n",
        "        # Exclamation marks (may indicate enthusiasm or overcompensation)\n",
        "        min(1.0, text.count('!') / 5.0),\n",
        "\n",
        "        # First-person pronouns (deceptive messages often use fewer)\n",
        "        len(re.findall(r'\\b(i|me|my|mine|myself|we|us|our|ours|ourselves)\\b', text)) / max(1, len(tokens)),\n",
        "\n",
        "        # Third-person pronouns (shifting focus away from self)\n",
        "        len(re.findall(r'\\b(he|she|they|them|their|his|her|theirs|himself|herself|themselves)\\b', text)) / max(1, len(tokens)),\n",
        "\n",
        "        # Tentative language (maybe, perhaps, guess, think)\n",
        "        len(re.findall(r'\\b(maybe|perhaps|possibly|guess|think|probably|about|approximately|around|might|could|may)\\b', text)) / max(1, len(tokens)),\n",
        "\n",
        "        # Certainty language (definitely, certainly, always, never)\n",
        "        len(re.findall(r'\\b(definitely|certainly|always|never|absolutely|completely|totally|surely|undoubtedly)\\b', text)) / max(1, len(tokens)),\n",
        "\n",
        "        # Non-specific language (things, stuff, something)\n",
        "        len(re.findall(r'\\b(thing|things|stuff|something|anything|everything|nothing|somewhere|anywhere|everywhere|nowhere)\\b', text)) / max(1, len(tokens)),\n",
        "\n",
        "        # Ratio of non-stopwords to total words (information density)\n",
        "        sum(1 for word in tokens if word not in stop_words) / max(1, len(tokens)),\n",
        "\n",
        "        # Average word length (can indicate complexity/sophistication)\n",
        "        sum(len(word) for word in tokens) / max(1, len(tokens))\n",
        "    ]\n",
        "\n",
        "    return np.array(features)\n",
        "\n",
        "# Extract message sentiment features\n",
        "def extract_sentiment_features(text):\n",
        "    \"\"\"Extract simple sentiment features\"\"\"\n",
        "    if not text:\n",
        "        return np.zeros(2)\n",
        "\n",
        "    positive_words = set([\n",
        "        'good', 'great', 'excellent', 'amazing', 'wonderful', 'fantastic', 'terrific',\n",
        "        'best', 'better', 'nice', 'happy', 'glad', 'pleased', 'delighted', 'love',\n",
        "        'like', 'enjoy', 'trust', 'honest', 'true', 'truth', 'agree', 'yes', 'perfect'\n",
        "    ])\n",
        "\n",
        "    negative_words = set([\n",
        "        'bad', 'terrible', 'awful', 'horrible', 'poor', 'wrong', 'worst',\n",
        "        'worse', 'sad', 'unhappy', 'angry', 'upset', 'hate', 'dislike',\n",
        "        'disagree', 'no', 'never', 'not', 'lie', 'lying', 'lied', 'fake', 'false'\n",
        "    ])\n",
        "\n",
        "    tokens = text.lower().split()\n",
        "    total_words = max(1, len(tokens))\n",
        "\n",
        "    positive_ratio = sum(1 for word in tokens if word in positive_words) / total_words\n",
        "    negative_ratio = sum(1 for word in tokens if word in negative_words) / total_words\n",
        "\n",
        "    return np.array([positive_ratio, negative_ratio])\n",
        "\n",
        "# Extract game state features\n",
        "def extract_game_state_features(item):\n",
        "    \"\"\"Extract features related to the game state\"\"\"\n",
        "    year = int(item['year'])\n",
        "    season = item['season']\n",
        "\n",
        "    # Game phase\n",
        "    early_game = 1.0 if year < 1905 else 0.0\n",
        "    mid_game = 1.0 if 1905 <= year < 1910 else 0.0\n",
        "    late_game = 1.0 if year >= 1910 else 0.0\n",
        "\n",
        "    # Season binary\n",
        "    is_spring = 1.0 if season == 'Spring' else 0.0\n",
        "    is_fall = 1.0 if season == 'Fall' else 0.0\n",
        "\n",
        "    # Communication intensity\n",
        "    rel_msg_idx = item['relative_message_index'] / 20.0  # Normalize\n",
        "    abs_msg_idx = item['absolute_message_index'] / 1000.0  # Normalize\n",
        "\n",
        "    # Score deltas (normalized)\n",
        "    score_delta = min(1.0, max(-1.0, int(item['score_delta']) / 5.0))\n",
        "    game_score = min(1.0, max(-1.0, int(item['game_score']) / 10.0))\n",
        "\n",
        "    return np.array([\n",
        "        early_game, mid_game, late_game,\n",
        "        is_spring, is_fall,\n",
        "        rel_msg_idx, abs_msg_idx,\n",
        "        score_delta, game_score\n",
        "    ])\n",
        "\n",
        "class ImprovedDiplomacyDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_len, player_to_idx, game_graphs=None,\n",
        "                 tfidf_vectorizer=None, bow_vectorizer=None):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.player_to_idx = player_to_idx\n",
        "        self.game_graphs = game_graphs\n",
        "        self.tfidf_vectorizer = tfidf_vectorizer\n",
        "        self.bow_vectorizer = bow_vectorizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "\n",
        "        # Get current message and context\n",
        "        current_message = item['current_message']\n",
        "        context_messages = item['context_messages']\n",
        "\n",
        "        # Preprocess text\n",
        "        current_message = preprocess_text(current_message)\n",
        "        context_messages = [preprocess_text(msg) for msg in context_messages]\n",
        "\n",
        "        # Combine context and current message with special tokens\n",
        "        full_text = \"[SEP]\".join(context_messages + [current_message])\n",
        "\n",
        "        # Tokenize text\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            full_text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Get sender and receiver indices\n",
        "        sender_idx = self.player_to_idx.get(item['sender'], 0)\n",
        "        receiver_idx = self.player_to_idx.get(item['receiver'], 0)\n",
        "\n",
        "        # Get game ID\n",
        "        game_id = item['game_id']\n",
        "\n",
        "        # Extract all features\n",
        "        ling_features = extract_linguistic_features(current_message)\n",
        "        sent_features = extract_sentiment_features(current_message)\n",
        "        game_features = extract_game_state_features(item)\n",
        "\n",
        "        # Prepare TF-IDF and BOW features if vectorizers are provided\n",
        "        tfidf_features = None\n",
        "        if self.tfidf_vectorizer:\n",
        "            tfidf_sparse = self.tfidf_vectorizer.transform([current_message])\n",
        "            tfidf_features = torch.tensor(tfidf_sparse.toarray()[0], dtype=torch.float32)\n",
        "\n",
        "        bow_features = None\n",
        "        if self.bow_vectorizer:\n",
        "            bow_sparse = self.bow_vectorizer.transform([current_message])\n",
        "            bow_features = torch.tensor(bow_sparse.toarray()[0], dtype=torch.float32)\n",
        "\n",
        "        # Comprehensive metadata features\n",
        "        metadata = torch.tensor(np.concatenate([\n",
        "            ling_features,\n",
        "            sent_features,\n",
        "            game_features\n",
        "        ]), dtype=torch.float32)\n",
        "\n",
        "        # Label\n",
        "        label = 1 if item['sender_label'] else 0\n",
        "\n",
        "        result = {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'sender_idx': torch.tensor(sender_idx, dtype=torch.long),\n",
        "            'receiver_idx': torch.tensor(receiver_idx, dtype=torch.long),\n",
        "            'metadata': metadata,\n",
        "            'game_id': game_id,\n",
        "            'label': torch.tensor(label, dtype=torch.long),\n",
        "            'text': current_message  # Store raw text for traditional models\n",
        "        }\n",
        "\n",
        "        # Add TF-IDF and BOW features if available\n",
        "        if tfidf_features is not None:\n",
        "            result['tfidf_features'] = tfidf_features\n",
        "\n",
        "        if bow_features is not None:\n",
        "            result['bow_features'] = bow_features\n",
        "\n",
        "        return result\n",
        "\n",
        "def build_enhanced_player_graphs(data_list):\n",
        "    \"\"\"Build enhanced player interaction graphs with more features\"\"\"\n",
        "    game_graphs = {}\n",
        "\n",
        "    # Group by game_id\n",
        "    game_data = defaultdict(list)\n",
        "    for item in data_list:\n",
        "        game_id = item['game_id']\n",
        "        game_data[game_id].append(item)\n",
        "\n",
        "    for game_id, game_items in game_data.items():\n",
        "        # Get all players in this game\n",
        "        players = list(set([item['sender'] for item in game_items] +\n",
        "                          [item['receiver'] for item in game_items]))\n",
        "        player_to_idx = {player: idx for idx, player in enumerate(players)}\n",
        "\n",
        "        # Initialize player statistics\n",
        "        player_stats = {player: {\n",
        "            'message_count': 0,\n",
        "            'deceptive_count': 0,\n",
        "            'countries_contacted': set(),\n",
        "            'avg_message_len': [],\n",
        "        } for player in players}\n",
        "\n",
        "        # Calculate player statistics\n",
        "        for item in game_items:\n",
        "            sender = item['sender']\n",
        "            receiver = item['receiver']\n",
        "            is_deceptive = item['sender_label']\n",
        "\n",
        "            player_stats[sender]['message_count'] += 1\n",
        "            if is_deceptive:\n",
        "                player_stats[sender]['deceptive_count'] += 1\n",
        "            player_stats[sender]['countries_contacted'].add(receiver)\n",
        "            player_stats[sender]['avg_message_len'].append(len(item['current_message'].split()))\n",
        "\n",
        "        # Create node features (richer player representations)\n",
        "        num_players = len(players)\n",
        "        node_features = []\n",
        "\n",
        "        for player in players:\n",
        "            stats = player_stats[player]\n",
        "            message_count = stats['message_count']\n",
        "\n",
        "            # Compute statistics\n",
        "            deception_ratio = stats['deceptive_count'] / max(1, message_count)\n",
        "            contact_diversity = len(stats['countries_contacted']) / max(1, len(players) - 1)\n",
        "            avg_msg_len = sum(stats['avg_message_len']) / max(1, len(stats['avg_message_len']))\n",
        "\n",
        "            # Normalize message length\n",
        "            norm_msg_len = min(1.0, avg_msg_len / 100.0)\n",
        "\n",
        "            # Create feature vector for this player\n",
        "            player_vector = [\n",
        "                deception_ratio,\n",
        "                contact_diversity,\n",
        "                norm_msg_len,\n",
        "                message_count / 100.0  # Normalize message count\n",
        "            ]\n",
        "\n",
        "            node_features.append(player_vector)\n",
        "\n",
        "        # Convert to tensor\n",
        "        x = torch.tensor(node_features, dtype=torch.float)\n",
        "\n",
        "        # Create edges (sender -> receiver) with enhanced edge features\n",
        "        edge_index = []\n",
        "        edge_attr = []  # Enhanced edge features\n",
        "\n",
        "        for item in game_items:\n",
        "            sender_idx = player_to_idx[item['sender']]\n",
        "            receiver_idx = player_to_idx[item['receiver']]\n",
        "\n",
        "            # Edge features\n",
        "            is_deceptive = 1.0 if item['sender_label'] else 0.0\n",
        "            message_length = min(1.0, len(item['current_message'].split()) / 100.0)\n",
        "            year_normalized = (int(item['year']) - 1900) / 30.0\n",
        "\n",
        "            # Game state features\n",
        "            is_spring = 1.0 if item['season'] == 'Spring' else 0.0\n",
        "            score_delta = min(1.0, max(-1.0, int(item['score_delta']) / 5.0))\n",
        "\n",
        "            edge_features = [\n",
        "                is_deceptive,\n",
        "                message_length,\n",
        "                year_normalized,\n",
        "                is_spring,\n",
        "                score_delta\n",
        "            ]\n",
        "\n",
        "            edge_index.append([sender_idx, receiver_idx])\n",
        "            edge_attr.append(edge_features)\n",
        "\n",
        "        if edge_index:  # Check if there are any edges\n",
        "            edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "            edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
        "\n",
        "            # Create PyTorch Geometric Data object\n",
        "            graph_data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
        "            game_graphs[game_id] = graph_data\n",
        "\n",
        "    return game_graphs\n",
        "\n",
        "def prepare_enhanced_data(jsonl_file):\n",
        "    \"\"\"Prepare enhanced data with more context and features\"\"\"\n",
        "    processed_data = []\n",
        "\n",
        "    with open(jsonl_file, 'r') as f:\n",
        "        for line in f:\n",
        "            game_data = json.loads(line)\n",
        "            messages = game_data['messages']\n",
        "            sender_labels = game_data['sender_labels']\n",
        "            speakers = game_data['speakers']\n",
        "            receivers = game_data['receivers']\n",
        "            rel_indices = game_data['relative_message_index']\n",
        "            abs_indices = game_data['absolute_message_index']\n",
        "            seasons = game_data['seasons']\n",
        "            years = game_data['years']\n",
        "            game_scores = game_data['game_score']\n",
        "            score_deltas = game_data['game_score_delta']\n",
        "            game_id = game_data['game_id']\n",
        "\n",
        "            # Parse messages by player for better context\n",
        "            player_messages = defaultdict(list)\n",
        "            for i in range(len(messages)):\n",
        "                if i < len(speakers) and i < len(messages):\n",
        "                    player = speakers[i]\n",
        "                    player_messages[player].append(i)\n",
        "\n",
        "            for i in range(len(messages)):\n",
        "                # Skip messages without labels\n",
        "                if i >= len(sender_labels) or sender_labels[i] == \"NOANNOTATION\":\n",
        "                    continue\n",
        "\n",
        "                # Get context (previous N messages overall)\n",
        "                context_start = max(0, i - NUM_PREVIOUS_MESSAGES)\n",
        "                general_context = messages[context_start:i]\n",
        "\n",
        "                # Also get the last message from this player if it exists\n",
        "                sender = speakers[i]\n",
        "                player_msg_indices = [idx for idx in player_messages[sender] if idx < i]\n",
        "\n",
        "                player_context = []\n",
        "                if player_msg_indices:\n",
        "                    # Get the most recent message from this player\n",
        "                    last_player_msg_idx = max(player_msg_indices)\n",
        "                    player_context = [messages[last_player_msg_idx]]\n",
        "\n",
        "                # Combine contexts with priority to player's own previous messages\n",
        "                context_messages = player_context + general_context\n",
        "                context_messages = context_messages[-NUM_PREVIOUS_MESSAGES:] if context_messages else []\n",
        "\n",
        "                # If there are fewer than N previous messages, pad with empty strings\n",
        "                while len(context_messages) < NUM_PREVIOUS_MESSAGES:\n",
        "                    context_messages.insert(0, \"\")\n",
        "\n",
        "                item = {\n",
        "                    'current_message': messages[i],\n",
        "                    'context_messages': context_messages,\n",
        "                    'sender': speakers[i],\n",
        "                    'receiver': receivers[i],\n",
        "                    'sender_label': sender_labels[i],\n",
        "                    'relative_message_index': rel_indices[i],\n",
        "                    'absolute_message_index': abs_indices[i],\n",
        "                    'season': seasons[i],\n",
        "                    'year': years[i],\n",
        "                    'game_score': game_scores[i],\n",
        "                    'score_delta': score_deltas[i],\n",
        "                    'game_id': game_id\n",
        "                }\n",
        "\n",
        "                processed_data.append(item)\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "def augment_enhanced_data(data_list):\n",
        "    \"\"\"Enhanced data augmentation with more sophisticated techniques\"\"\"\n",
        "    augmented_data = []\n",
        "\n",
        "    # Focus on augmenting the minority class (usually truthful messages)\n",
        "    truthful_data = [item for item in data_list if not item['sender_label']]\n",
        "    deceptive_data = [item for item in data_list if item['sender_label']]\n",
        "\n",
        "    truthful_count = len(truthful_data)\n",
        "    deceptive_count = len(deceptive_data)\n",
        "    print(f\"Original data - Truthful: {truthful_count}, Deceptive: {deceptive_count}\")\n",
        "\n",
        "    # Determine which class is minority\n",
        "    minority_class = truthful_data if truthful_count < deceptive_count else deceptive_data\n",
        "    majority_class = deceptive_data if truthful_count < deceptive_count else truthful_data\n",
        "\n",
        "    minority_label = False if truthful_count < deceptive_count else True\n",
        "\n",
        "    print(f\"Minority class: {'Truthful' if truthful_count < deceptive_count else 'Deceptive'}\")\n",
        "\n",
        "    # Calculate augmentation factor to balance classes\n",
        "    target_count = len(majority_class)\n",
        "    needed_augmentations = max(0, target_count - len(minority_class))\n",
        "    augmentation_factor = max(1, needed_augmentations // len(minority_class) + 1)\n",
        "\n",
        "    print(f\"Creating {augmentation_factor} augmentations per minority class message\")\n",
        "\n",
        "    # Advanced augmentation techniques\n",
        "    def swap_words(text, swap_ratio=0.15):\n",
        "        words = text.split()\n",
        "        if len(words) <= 3:\n",
        "            return text\n",
        "        num_swaps = max(1, int(swap_ratio * len(words)))\n",
        "        for _ in range(num_swaps):\n",
        "            if len(words) >= 2:\n",
        "                idx1, idx2 = random.sample(range(len(words)), 2)\n",
        "                words[idx1], words[idx2] = words[idx2], words[idx1]\n",
        "        return ' '.join(words)\n",
        "\n",
        "    def synonym_replacement(text):\n",
        "        # Expanded synonym dictionary\n",
        "        synonyms = {\n",
        "            'good': ['great', 'excellent', 'fine', 'positive', 'beneficial'],\n",
        "            'bad': ['poor', 'terrible', 'negative', 'awful', 'undesirable'],\n",
        "            'attack': ['assault', 'strike', 'hit', 'offensive', 'charge'],\n",
        "            'defend': ['protect', 'guard', 'shield', 'cover', 'safeguard'],\n",
        "            'ally': ['partner', 'friend', 'supporter', 'confederate', 'associate'],\n",
        "            'enemy': ['opponent', 'adversary', 'foe', 'rival', 'antagonist'],\n",
        "            'think': ['believe', 'consider', 'feel', 'reckon', 'suppose'],\n",
        "            'move': ['advance', 'proceed', 'shift', 'progress', 'transfer'],\n",
        "            'help': ['assist', 'aid', 'support', 'back', 'facilitate'],\n",
        "            'plan': ['strategy', 'scheme', 'idea', 'approach', 'method'],\n",
        "            'want': ['need', 'desire', 'wish', 'hope', 'prefer'],\n",
        "            'take': ['grab', 'seize', 'capture', 'acquire', 'secure'],\n",
        "            'give': ['provide', 'offer', 'supply', 'deliver', 'furnish'],\n",
        "            'make': ['create', 'produce', 'form', 'construct', 'develop'],\n",
        "            'see': ['observe', 'notice', 'spot', 'witness', 'perceive'],\n",
        "            'say': ['tell', 'mention', 'state', 'declare', 'express'],\n",
        "            'know': ['understand', 'recognize', 'realize', 'comprehend', 'grasp']\n",
        "        }\n",
        "\n",
        "        words = text.split()\n",
        "        for i, word in enumerate(words):\n",
        "            word_lower = word.lower()\n",
        "            if word_lower in synonyms and random.random() < 0.3:\n",
        "                words[i] = random.choice(synonyms[word_lower])\n",
        "\n",
        "        return ' '.join(words)\n",
        "\n",
        "    def random_insertion(text):\n",
        "        \"\"\"Insert random common words in the text\"\"\"\n",
        "        common_inserts = [\n",
        "            'I think', 'perhaps', 'maybe', 'actually', 'honestly',\n",
        "            'of course', 'clearly', 'anyway', 'definitely', 'absolutely',\n",
        "            'in fact', 'obviously', 'frankly', 'truthfully'\n",
        "        ]\n",
        "\n",
        "        words = text.split()\n",
        "        if len(words) <= 2:\n",
        "            return text\n",
        "\n",
        "        insert_pos = random.randint(0, len(words) - 1)\n",
        "        insert_word = random.choice(common_inserts)\n",
        "        words.insert(insert_pos, insert_word)\n",
        "\n",
        "        return ' '.join(words)\n",
        "\n",
        "    def random_deletion(text, p=0.1):\n",
        "        \"\"\"Randomly delete words from the text\"\"\"\n",
        "        words = text.split()\n",
        "        if len(words) <= 3:\n",
        "            return text\n",
        "\n",
        "        kept_words = []\n",
        "        for word in words:\n",
        "            if random.random() > p:\n",
        "                kept_words.append(word)\n",
        "\n",
        "        if not kept_words:  # Make sure we keep at least one word\n",
        "            kept_words = [random.choice(words)]\n",
        "\n",
        "        return ' '.join(kept_words)\n",
        "\n",
        "    def add_noise(text, noise_level=0.1):\n",
        "        words = text.split()\n",
        "        if len(words) <= 3:\n",
        "            return text\n",
        "\n",
        "        # Randomly skip some words (mimics typos/omissions)\n",
        "        if random.random() < noise_level:\n",
        "            skip_idx = random.randint(0, len(words)-1)\n",
        "            words.pop(skip_idx)\n",
        "\n",
        "        # Randomly add fillers\n",
        "        fillers = ['um', 'well', 'like', 'you know', 'I mean', 'so', 'anyway', 'basically']\n",
        "        if random.random() < noise_level and len(words) > 3:\n",
        "            insert_idx = random.randint(0, len(words)-1)\n",
        "            words.insert(insert_idx, random.choice(fillers))\n",
        "\n",
        "        return ' '.join(words)\n",
        "\n",
        "    # Apply augmentation to minority class messages\n",
        "    for item in minority_class:\n",
        "        current_message = item['current_message']\n",
        "\n",
        "        # Skip very short messages\n",
        "        if not current_message or len(current_message.split()) < 3:\n",
        "            continue\n",
        "\n",
        "        for i in range(augmentation_factor):\n",
        "            augmented_item = item.copy()\n",
        "\n",
        "            # Apply different augmentation strategies\n",
        "            if i % 5 == 0:\n",
        "                augmented_text = swap_words(current_message)\n",
        "            elif i % 5 == 1:\n",
        "                augmented_text = synonym_replacement(current_message)\n",
        "            elif i % 5 == 2:\n",
        "                augmented_text = random_insertion(current_message)\n",
        "            elif i % 5 == 3:\n",
        "                augmented_text = random_deletion(current_message)\n",
        "            else:\n",
        "                augmented_text = add_noise(current_message)\n",
        "\n",
        "            augmented_item['current_message'] = augmented_text\n",
        "            augmented_data.append(augmented_item)\n",
        "\n",
        "    # Combine original and augmented data\n",
        "    combined_data = data_list + augmented_data\n",
        "\n",
        "    # Count classes after augmentation\n",
        "    truthful_count_after = sum(1 for item in combined_data if not item['sender_label'])\n",
        "    deceptive_count_after = sum(1 for item in combined_data if item['sender_label'])\n",
        "    print(f\"After augmentation - Truthful: {truthful_count_after}, Deceptive: {deceptive_count_after}\")\n",
        "\n",
        "    return combined_data\n",
        "\n",
        "class EnhancedGNNModule(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(EnhancedGNNModule, self).__init__()\n",
        "        # GCN layer for initial feature processing\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "\n",
        "        # GAT layers for attention-based message passing\n",
        "        self.gat1 = GATConv(hidden_dim, hidden_dim // 2, heads=2, dropout=DROPOUT_RATE)\n",
        "        self.gat2 = GATv2Conv(hidden_dim, output_dim, heads=1, dropout=DROPOUT_RATE)\n",
        "\n",
        "        self.dropout = nn.Dropout(DROPOUT_RATE)\n",
        "        self.layer_norm1 = nn.LayerNorm(hidden_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr=None):\n",
        "        # Initial convolution\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "        x = self.layer_norm1(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # First GAT layer with multi-head attention\n",
        "        x = F.elu(self.gat1(x, edge_index))\n",
        "        x = self.layer_norm2(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Second GAT layer\n",
        "        x = self.gat2(x, edge_index)\n",
        "\n",
        "        return x\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2.0):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        BCE_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
        "        pt = torch.exp(-BCE_loss)\n",
        "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
        "        return F_loss.mean()\n",
        "\n",
        "class EnhancedDeceptionModel(nn.Module):\n",
        "    def __init__(self, num_players, player_embedding_dim, metadata_dim, tfidf_dim=None, bow_dim=None):\n",
        "        super(EnhancedDeceptionModel, self).__init__()\n",
        "\n",
        "        # Use a stronger transformer model\n",
        "        self.bert = RobertaModel.from_pretrained(\"roberta-base\")\n",
        "        self.bert_dim = self.bert.config.hidden_size  # 768 for roberta-base\n",
        "\n",
        "        # Freeze part of the transformer to prevent overfitting\n",
        "        # Only fine-tune the top 4 layers\n",
        "        for param in list(self.bert.parameters())[:-4 * len(list(self.bert.encoder.layer[-1].parameters()))]:\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Enhanced GNN for player relationship modeling\n",
        "        self.gnn = EnhancedGNNModule(num_players + 4, 128, player_embedding_dim)  # +4 for additional node features\n",
        "\n",
        "        # Player embeddings as backup when graph unavailable\n",
        "        self.player_embeddings = nn.Embedding(num_players, player_embedding_dim)\n",
        "\n",
        "        # Metadata projection with layer normalization\n",
        "        self.metadata_projection = nn.Sequential(\n",
        "            nn.Linear(21, metadata_dim),  # Updated for additional features\n",
        "            nn.LayerNorm(metadata_dim),\n",
        "            nn.Dropout(DROPOUT_RATE),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # TF-IDF and BOW projections with regularization\n",
        "        self.tfidf_dim = tfidf_dim\n",
        "        if tfidf_dim is not None:\n",
        "            self.tfidf_projection = nn.Sequential(\n",
        "                nn.Linear(tfidf_dim, 64),\n",
        "                nn.LayerNorm(64),\n",
        "                nn.Dropout(DROPOUT_RATE),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "\n",
        "        self.bow_dim = bow_dim\n",
        "        if bow_dim is not None:\n",
        "            self.bow_projection = nn.Sequential(\n",
        "                nn.Linear(bow_dim, 64),\n",
        "                nn.LayerNorm(64),\n",
        "                nn.Dropout(DROPOUT_RATE),\n",
        "                nn.ReLU()\n",
        "            )\n",
        "\n",
        "        # Calculate fusion input dimension\n",
        "        fusion_input_dim = self.bert_dim + player_embedding_dim * 2 + metadata_dim\n",
        "        if tfidf_dim is not None:\n",
        "            fusion_input_dim += 64\n",
        "        if bow_dim is not None:\n",
        "            fusion_input_dim += 64\n",
        "\n",
        "        # Enhanced fusion layer with residual connections\n",
        "        self.fusion_layer1 = nn.Linear(fusion_input_dim, 256)\n",
        "        self.layer_norm1 = nn.LayerNorm(256)\n",
        "        self.fusion_layer2 = nn.Linear(256, 128)\n",
        "        self.layer_norm2 = nn.LayerNorm(128)\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(128, 2)\n",
        "\n",
        "        # Additional regularization\n",
        "        self.dropout = nn.Dropout(DROPOUT_RATE)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, sender_idx, receiver_idx, metadata, game_id=None,\n",
        "                tfidf_features=None, bow_features=None, game_graphs=None):\n",
        "        # Process text with BERT\n",
        "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = bert_output.pooler_output\n",
        "\n",
        "        # Process metadata\n",
        "        metadata_features = self.metadata_projection(metadata)\n",
        "\n",
        "        # Process sender and receiver embeddings\n",
        "        # If game graph is available, use GNN-processed embeddings\n",
        "        sender_embed = None\n",
        "        receiver_embed = None\n",
        "\n",
        "        if game_graphs is not None and game_id in game_graphs:\n",
        "            graph = game_graphs[game_id]\n",
        "            node_features = graph.x\n",
        "            edge_index = graph.edge_index\n",
        "            edge_attr = graph.edge_attr if hasattr(graph, 'edge_attr') else None\n",
        "\n",
        "            # Get node embeddings from GNN\n",
        "            node_embeddings = self.gnn(node_features, edge_index, edge_attr)\n",
        "\n",
        "            # Extract embeddings for sender and receiver\n",
        "            sender_embed = node_embeddings[sender_idx]\n",
        "            receiver_embed = node_embeddings[receiver_idx]\n",
        "        else:\n",
        "            # Fallback to regular embeddings if graph not available\n",
        "            sender_embed = self.player_embeddings(sender_idx)\n",
        "            receiver_embed = self.player_embeddings(receiver_idx)\n",
        "\n",
        "        # Process TF-IDF and BOW features if available\n",
        "        tfidf_projected = None\n",
        "        if self.tfidf_dim is not None and tfidf_features is not None:\n",
        "            tfidf_projected = self.tfidf_projection(tfidf_features)\n",
        "\n",
        "        bow_projected = None\n",
        "        if self.bow_dim is not None and bow_features is not None:\n",
        "            bow_projected = self.bow_projection(bow_features)\n",
        "\n",
        "        # Concatenate all features\n",
        "        combined = [pooled_output, sender_embed, receiver_embed, metadata_features]\n",
        "        if tfidf_projected is not None:\n",
        "            combined.append(tfidf_projected)\n",
        "        if bow_projected is not None:\n",
        "            combined.append(bow_projected)\n",
        "\n",
        "        fused = torch.cat(combined, dim=1)\n",
        "\n",
        "        # Pass through fusion layers with residual connections\n",
        "        hidden1 = self.fusion_layer1(fused)\n",
        "        hidden1 = self.layer_norm1(hidden1)\n",
        "        hidden1 = F.relu(hidden1)\n",
        "        hidden1 = self.dropout(hidden1)\n",
        "\n",
        "        hidden2 = self.fusion_layer2(hidden1)\n",
        "        hidden2 = self.layer_norm2(hidden2)\n",
        "        hidden2 = F.relu(hidden2)\n",
        "        hidden2 = self.dropout(hidden2)\n",
        "\n",
        "        # Output\n",
        "        logits = self.output_layer(hidden2)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
        "                num_epochs, device, game_graphs=None, early_stopping_patience=3):\n",
        "    \"\"\"Train the model with extensive logging and early stopping\"\"\"\n",
        "    best_val_f1 = 0.0\n",
        "    no_improvement_count = 0\n",
        "    training_history = {'train_loss': [], 'train_acc': [], 'train_f1': [],\n",
        "                       'val_loss': [], 'val_acc': [], 'val_f1': []}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_preds = []\n",
        "        train_labels = []\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=\"Training\"):\n",
        "            # Move batch to device\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            sender_idx = batch['sender_idx'].to(device)\n",
        "            receiver_idx = batch['receiver_idx'].to(device)\n",
        "            metadata = batch['metadata'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            game_ids = batch['game_id']\n",
        "\n",
        "            # Get optional features if available\n",
        "            tfidf_features = batch.get('tfidf_features')\n",
        "            if tfidf_features is not None:\n",
        "                tfidf_features = tfidf_features.to(device)\n",
        "\n",
        "            bow_features = batch.get('bow_features')\n",
        "            if bow_features is not None:\n",
        "                bow_features = bow_features.to(device)\n",
        "\n",
        "            # Clear gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                sender_idx=sender_idx,\n",
        "                receiver_idx=receiver_idx,\n",
        "                metadata=metadata,\n",
        "                game_id=game_ids,\n",
        "                tfidf_features=tfidf_features,\n",
        "                bow_features=bow_features,\n",
        "                game_graphs=game_graphs\n",
        "            )\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            # Gradient clipping to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update statistics\n",
        "            train_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            train_preds.extend(preds.cpu().numpy())\n",
        "            train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate training metrics\n",
        "        train_loss = train_loss / len(train_loader)\n",
        "        train_acc = accuracy_score(train_labels, train_preds)\n",
        "        train_f1 = f1_score(train_labels, train_preds, average='macro')\n",
        "\n",
        "        # Evaluation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_preds = []\n",
        "        val_labels = []\n",
        "        val_probs = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=\"Validation\"):\n",
        "                # Move batch to device\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                sender_idx = batch['sender_idx'].to(device)\n",
        "                receiver_idx = batch['receiver_idx'].to(device)\n",
        "                metadata = batch['metadata'].to(device)\n",
        "                labels = batch['label'].to(device)\n",
        "                game_ids = batch['game_id']\n",
        "\n",
        "                # Get optional features if available\n",
        "                tfidf_features = batch.get('tfidf_features')\n",
        "                if tfidf_features is not None:\n",
        "                    tfidf_features = tfidf_features.to(device)\n",
        "\n",
        "                bow_features = batch.get('bow_features')\n",
        "                if bow_features is not None:\n",
        "                    bow_features = bow_features.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    sender_idx=sender_idx,\n",
        "                    receiver_idx=receiver_idx,\n",
        "                    metadata=metadata,\n",
        "                    game_id=game_ids,\n",
        "                    tfidf_features=tfidf_features,\n",
        "                    bow_features=bow_features,\n",
        "                    game_graphs=game_graphs\n",
        "                )\n",
        "\n",
        "                # Calculate loss\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Update statistics\n",
        "                val_loss += loss.item()\n",
        "                probs = F.softmax(outputs, dim=1)\n",
        "                val_probs.extend(probs[:, 1].cpu().numpy())\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                val_preds.extend(preds.cpu().numpy())\n",
        "                val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate validation metrics\n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_acc = accuracy_score(val_labels, val_preds)\n",
        "        val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Print metrics\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
        "\n",
        "        # Update history\n",
        "        training_history['train_loss'].append(train_loss)\n",
        "        training_history['train_acc'].append(train_acc)\n",
        "        training_history['train_f1'].append(train_f1)\n",
        "        training_history['val_loss'].append(val_loss)\n",
        "        training_history['val_acc'].append(val_acc)\n",
        "        training_history['val_f1'].append(val_f1)\n",
        "\n",
        "        # Early stopping check\n",
        "        if val_f1 > best_val_f1:\n",
        "            best_val_f1 = val_f1\n",
        "            no_improvement_count = 0\n",
        "            # Save the best model\n",
        "            print(\"Saving new best model...\")\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "        else:\n",
        "            no_improvement_count += 1\n",
        "            if no_improvement_count >= early_stopping_patience:\n",
        "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "                break\n",
        "\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    # Plot training history\n",
        "    plot_training_history(training_history)\n",
        "\n",
        "    return training_history\n",
        "\n",
        "\n",
        "def evaluate_model(model, test_loader, criterion, device, game_graphs=None):\n",
        "    \"\"\"Evaluate the model on the test set with detailed metrics\"\"\"\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    test_preds = []\n",
        "    test_labels = []\n",
        "    test_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
        "            # Move batch to device\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            sender_idx = batch['sender_idx'].to(device)\n",
        "            receiver_idx = batch['receiver_idx'].to(device)\n",
        "            metadata = batch['metadata'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            game_ids = batch['game_id']\n",
        "\n",
        "            # Get optional features if available\n",
        "            tfidf_features = batch.get('tfidf_features')\n",
        "            if tfidf_features is not None:\n",
        "                tfidf_features = tfidf_features.to(device)\n",
        "\n",
        "            bow_features = batch.get('bow_features')\n",
        "            if bow_features is not None:\n",
        "                bow_features = bow_features.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                sender_idx=sender_idx,\n",
        "                receiver_idx=receiver_idx,\n",
        "                metadata=metadata,\n",
        "                game_id=game_ids,\n",
        "                tfidf_features=tfidf_features,\n",
        "                bow_features=bow_features,\n",
        "                game_graphs=game_graphs\n",
        "            )\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Update statistics\n",
        "            test_loss += loss.item()\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            test_probs.extend(probs[:, 1].cpu().numpy())\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            test_preds.extend(preds.cpu().numpy())\n",
        "            test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate average loss\n",
        "    test_loss = test_loss / len(test_loader)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(test_labels, test_preds)\n",
        "    f1 = f1_score(test_labels, test_preds, average='macro')\n",
        "    roc_auc = roc_auc_score(test_labels, test_probs)\n",
        "\n",
        "    # Print detailed classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(test_labels, test_preds, target_names=['Truthful', 'Deceptive']))\n",
        "\n",
        "    # Plot PR and ROC curves\n",
        "    plot_evaluation_curves(test_labels, test_probs)\n",
        "\n",
        "    # Print summary metrics\n",
        "    print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Test Macro F1: {f1:.4f}\")\n",
        "    print(f\"Test ROC AUC: {roc_auc:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'loss': test_loss,\n",
        "        'accuracy': accuracy,\n",
        "        'f1_score': f1,\n",
        "        'roc_auc': roc_auc,\n",
        "        'predictions': test_preds,\n",
        "        'probabilities': test_probs,\n",
        "        'true_labels': test_labels\n",
        "    }\n",
        "\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"Plot training and validation metrics over epochs\"\"\"\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.subplot(3, 1, 1)\n",
        "    plt.plot(history['train_loss'], label='Train Loss')\n",
        "    plt.plot(history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Loss over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot accuracy\n",
        "    plt.subplot(3, 1, 2)\n",
        "    plt.plot(history['train_acc'], label='Train Accuracy')\n",
        "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
        "    plt.title('Accuracy over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot F1 score\n",
        "    plt.subplot(3, 1, 3)\n",
        "    plt.plot(history['train_f1'], label='Train F1')\n",
        "    plt.plot(history['val_f1'], label='Validation F1')\n",
        "    plt.title('F1 Score over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('F1 Score')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_history.png')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_evaluation_curves(true_labels, probabilities):\n",
        "    \"\"\"Plot Precision-Recall and ROC curves\"\"\"\n",
        "    plt.figure(figsize=(15, 6))\n",
        "\n",
        "    # Precision-Recall curve\n",
        "    plt.subplot(1, 2, 1)\n",
        "    precision, recall, _ = precision_recall_curve(true_labels, probabilities)\n",
        "    plt.plot(recall, precision, marker='.')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # ROC curve\n",
        "    plt.subplot(1, 2, 2)\n",
        "    fpr, tpr, _ = precision_recall_curve(true_labels, probabilities)\n",
        "    plt.plot(fpr, tpr, marker='.')\n",
        "    plt.title('ROC Curve')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('evaluation_curves.png')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def analyze_errors(true_labels, predictions, probabilities, texts, senders, receivers, game_ids):\n",
        "    \"\"\"Analyze the model's errors to gain insights\"\"\"\n",
        "    # Find the indices of false positives and false negatives\n",
        "    fp_indices = np.where((predictions == 1) & (np.array(true_labels) == 0))[0]\n",
        "    fn_indices = np.where((predictions == 0) & (np.array(true_labels) == 1))[0]\n",
        "\n",
        "    # Print some examples of false positives\n",
        "    print(\"\\nFalse Positive Examples (Truthful messages classified as Deceptive):\")\n",
        "    for i in range(min(5, len(fp_indices))):\n",
        "        idx = fp_indices[i]\n",
        "        print(f\"Game ID: {game_ids[idx]}\")\n",
        "        print(f\"Sender: {senders[idx]}, Receiver: {receivers[idx]}\")\n",
        "        print(f\"Text: {texts[idx]}\")\n",
        "        print(f\"Confidence: {probabilities[idx]:.4f}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "    # Print some examples of false negatives\n",
        "    print(\"\\nFalse Negative Examples (Deceptive messages classified as Truthful):\")\n",
        "    for i in range(min(5, len(fn_indices))):\n",
        "        idx = fn_indices[i]\n",
        "        print(f\"Game ID: {game_ids[idx]}\")\n",
        "        print(f\"Sender: {senders[idx]}, Receiver: {receivers[idx]}\")\n",
        "        print(f\"Text: {texts[idx]}\")\n",
        "        print(f\"Confidence: {1 - probabilities[idx]:.4f}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "    # Analyze errors by game id, sender, and receiver\n",
        "    error_by_game = {}\n",
        "    error_by_sender = {}\n",
        "    error_by_receiver = {}\n",
        "\n",
        "    for idx in range(len(true_labels)):\n",
        "        game_id = game_ids[idx]\n",
        "        sender = senders[idx]\n",
        "        receiver = receivers[idx]\n",
        "        is_error = predictions[idx] != true_labels[idx]\n",
        "\n",
        "        # Update error counts\n",
        "        if game_id not in error_by_game:\n",
        "            error_by_game[game_id] = {'total': 0, 'errors': 0}\n",
        "        error_by_game[game_id]['total'] += 1\n",
        "        if is_error:\n",
        "            error_by_game[game_id]['errors'] += 1\n",
        "\n",
        "        if sender not in error_by_sender:\n",
        "            error_by_sender[sender] = {'total': 0, 'errors': 0}\n",
        "        error_by_sender[sender]['total'] += 1\n",
        "        if is_error:\n",
        "            error_by_sender[sender]['errors'] += 1\n",
        "\n",
        "        if receiver not in error_by_receiver:\n",
        "            error_by_receiver[receiver] = {'total': 0, 'errors': 0}\n",
        "        error_by_receiver[receiver]['total'] += 1\n",
        "        if is_error:\n",
        "            error_by_receiver[receiver]['errors'] += 1\n",
        "\n",
        "    # Calculate error rates\n",
        "    for game_id, counts in error_by_game.items():\n",
        "        counts['error_rate'] = counts['errors'] / counts['total']\n",
        "\n",
        "    for sender, counts in error_by_sender.items():\n",
        "        counts['error_rate'] = counts['errors'] / counts['total']\n",
        "\n",
        "    for receiver, counts in error_by_receiver.items():\n",
        "        counts['error_rate'] = counts['errors'] / counts['total']\n",
        "\n",
        "    # Print error rates\n",
        "    print(\"\\nError rates by game:\")\n",
        "    for game_id, counts in sorted(error_by_game.items(), key=lambda x: x[1]['error_rate'], reverse=True):\n",
        "        print(f\"Game {game_id}: {counts['error_rate']:.4f} ({counts['errors']}/{counts['total']})\")\n",
        "\n",
        "    print(\"\\nError rates by sender:\")\n",
        "    for sender, counts in sorted(error_by_sender.items(), key=lambda x: x[1]['error_rate'], reverse=True):\n",
        "        print(f\"Sender {sender}: {counts['error_rate']:.4f} ({counts['errors']}/{counts['total']})\")\n",
        "\n",
        "    print(\"\\nError rates by receiver:\")\n",
        "    for receiver, counts in sorted(error_by_receiver.items(), key=lambda x: x[1]['error_rate'], reverse=True):\n",
        "        print(f\"Receiver {receiver}: {counts['error_rate']:.4f} ({counts['errors']}/{counts['total']})\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Load data\n",
        "    print(\"Loading data...\")\n",
        "    train_data = prepare_enhanced_data('train.jsonl')\n",
        "    val_data = prepare_enhanced_data('validation.jsonl')\n",
        "    test_data = prepare_enhanced_data('test.jsonl')\n",
        "\n",
        "    # Augment the training data for better balance\n",
        "    print(\"Augmenting training data...\")\n",
        "    augmented_train_data = augment_enhanced_data(train_data)\n",
        "\n",
        "    # Build player embeddings\n",
        "    print(\"Building player mappings...\")\n",
        "    all_players = set()\n",
        "    for item in train_data + val_data + test_data:\n",
        "        all_players.add(item['sender'])\n",
        "        all_players.add(item['receiver'])\n",
        "\n",
        "    player_to_idx = {player: idx + 1 for idx, player in enumerate(sorted(all_players))}\n",
        "    player_to_idx['UNK'] = 0  # Add unknown player token\n",
        "\n",
        "    # Build TF-IDF and BOW vectorizers\n",
        "    print(\"Building TF-IDF and BOW features...\")\n",
        "    train_texts = [item['current_message'] for item in augmented_train_data]\n",
        "\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=1000, min_df=5, max_df=0.7)\n",
        "    tfidf_vectorizer.fit(train_texts)\n",
        "\n",
        "    bow_vectorizer = CountVectorizer(max_features=1000, min_df=5, max_df=0.7)\n",
        "    bow_vectorizer.fit(train_texts)\n",
        "\n",
        "    # Build player graphs\n",
        "    print(\"Building player interaction graphs...\")\n",
        "    game_graphs = build_enhanced_player_graphs(augmented_train_data + val_data + test_data)\n",
        "\n",
        "    # Load tokenizer\n",
        "    print(\"Loading tokenizer...\")\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "    # Create datasets\n",
        "    print(\"Creating datasets...\")\n",
        "    train_dataset = ImprovedDiplomacyDataset(\n",
        "        augmented_train_data, tokenizer, MAX_LEN, player_to_idx, game_graphs,\n",
        "        tfidf_vectorizer, bow_vectorizer\n",
        "    )\n",
        "\n",
        "    val_dataset = ImprovedDiplomacyDataset(\n",
        "        val_data, tokenizer, MAX_LEN, player_to_idx, game_graphs,\n",
        "        tfidf_vectorizer, bow_vectorizer\n",
        "    )\n",
        "\n",
        "    test_dataset = ImprovedDiplomacyDataset(\n",
        "        test_data, tokenizer, MAX_LEN, player_to_idx, game_graphs,\n",
        "        tfidf_vectorizer, bow_vectorizer\n",
        "    )\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Create model\n",
        "    print(\"Initializing model...\")\n",
        "    model = EnhancedDeceptionModel(\n",
        "        num_players=len(player_to_idx),\n",
        "        player_embedding_dim=PLAYER_EMBEDDING_DIM,\n",
        "        metadata_dim=METADATA_DIM,\n",
        "        tfidf_dim=len(tfidf_vectorizer.vocabulary_),\n",
        "        bow_dim=len(bow_vectorizer.vocabulary_)\n",
        "    )\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = FocalLoss(alpha=0.25, gamma=2.0)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)\n",
        "\n",
        "    # Train model\n",
        "    print(\"Training model...\")\n",
        "    history = train_model(\n",
        "        model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
        "        num_epochs=EPOCHS, device=device, game_graphs=game_graphs\n",
        "    )\n",
        "\n",
        "    # Load best model\n",
        "    print(\"Loading best model...\")\n",
        "    model.load_state_dict(torch.load('best_model.pt'))\n",
        "\n",
        "    # Evaluate on test set\n",
        "    print(\"Evaluating on test set...\")\n",
        "    test_results = evaluate_model(model, test_loader, criterion, device, game_graphs)\n",
        "\n",
        "    # Analyze errors\n",
        "    print(\"Analyzing errors...\")\n",
        "    test_texts = [item['current_message'] for item in test_data]\n",
        "    test_senders = [item['sender'] for item in test_data]\n",
        "    test_receivers = [item['receiver'] for item in test_data]\n",
        "    test_game_ids = [item['game_id'] for item in test_data]\n",
        "\n",
        "    analyze_errors(\n",
        "        test_results['true_labels'],\n",
        "        test_results['predictions'],\n",
        "        test_results['probabilities'],\n",
        "        test_texts,\n",
        "        test_senders,\n",
        "        test_receivers,\n",
        "        test_game_ids\n",
        "    )\n",
        "\n",
        "    # Save the final model\n",
        "    print(\"Saving model...\")\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'player_to_idx': player_to_idx,\n",
        "        'tfidf_vectorizer': tfidf_vectorizer,\n",
        "        'bow_vectorizer': bow_vectorizer\n",
        "    }, 'diplomacy_deception_model_complete.pt')\n",
        "\n",
        "    print(\"Done!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}